{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD4DRm15ahAE3BahmUXJ2b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JyothiSupriya/Generative_AI_Course/blob/main/Generative_AI_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction:\n",
        "Deep Learning can be divided into the following segments:\n",
        "1. ANN - Artifical Neural Networks\n",
        "2. RNN - Recurrent Neural Networks (Sequance related data) - Feedback Loop in hidden layer  \n",
        "3. CNN - Convolutional Neural Networks (Images or Videos types of Data) - Feature Extraction (Filter) + Pooling + Faltern + ANN\n",
        "4. RL - Reinforment Learning - Agent - Environment - State , etc.,\n",
        "5. GAN's - Genarative Adverserial Networks - Generator and Discriminator - look at the Architecture.\n",
        "\n",
        "Input Layer -------> Hidden Layer -----> Output Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "a3xujovAExaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Generative AI?\n",
        "Generative AI Generates new data based on training sample. Generative model can generate Images, Text, Audio, Videos,  etc., Data as Output.\n",
        "\n",
        "So generative AI is a very huge topics,\n",
        "- Generative Images model. - (Images to Images)\n",
        "- Generative Language model. (Text to Images & Text to Text)\n"
      ],
      "metadata": {
        "id": "efuh81UwLuav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Transformer Tree:\n",
        " Encoder:\n",
        " BERT, RoBERTa, XLM, ALBERT, ELECTRA, DeBERTa.\n",
        " Both:\n",
        " T5, BART, M2M-100, BigBird.\n",
        " Decoder:\n",
        " GPT, GPT-2, GPT-3, GPT-Neo."
      ],
      "metadata": {
        "id": "witl9_-SNUqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Internet text data + Documents text data) Stage -1 (Generative Pre-training): This base model thingy (GPT) that was trained on a bunch of stuff from the internet for a whole bunch of different things by using the tranformer architecture.\n",
        "Gives the Base GPT model\n",
        "Stage  -2(Supervised Fine Tuning(SFT)): Next, with the human AI trainers, you get to have conversations where they play both sides.\n",
        "Gives Fine-tuned ChatGPT model.\n",
        "Stage-3(Reinforment Learning through human feedback(RLHF)): Next, let's take the modle to the nect level by optimizing it even more wit Refinforcement Learning it against a reward model.\n",
        "Gives ChatGPT\n"
      ],
      "metadata": {
        "id": "ZLc_0ko027Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Models and OpenAI\n",
        "\n",
        "chatGPT playground"
      ],
      "metadata": {
        "id": "Q-kdGFcE-BeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7aK1IHLu2hS",
        "outputId": "556d7f89-34b6-45cb-a98c-1a65cfc2d34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.0-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "yLx6HrnMjFO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Completion API and Function Calling:\n",
        "openai.Completion.create(): This method is used to generate completions or responses. You provide a series of mnessages as input, and the API generates a model-generated messages as output.\n",
        "\n",
        "openai.ChatCompletion.create(): Similar to Completion.create(), but specifically designed for chat-based language models. it takes a sereis of messafes as input and generates a model-generated messages as output."
      ],
      "metadata": {
        "id": "jlT-Py4S-dPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code is for v1 of the openai package: pypi.org/project/openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "respose = client.Completion.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages =\n",
        "    \"role\":\"system\",\n",
        "    content = \"You are a helpful assistant that translates English to French.\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "WozYN_rf_Ekw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Calling in OpenAPI: we create a function and than call it, it is similar to the class: functions in the programming. we can also call API here for getting information from the internet (for example: Rapid API)\n",
        "Function Calling: Learn how to connect language models to external tools.\n",
        "\n",
        "Few-Shot and Zero-Shot Learning: Few-shot learning is giving a source fro which the data should be extracted and Zero-shot learning is asking a question dirtectly to the model."
      ],
      "metadata": {
        "id": "6UaEgYJRQ-V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain: it is a wrapper around OpenAI or any other LLM's. So, here when we request any information from the LLM's, it won't go directly to the LLM'S but it will be passed through langchain and then redirected to the LLM's.\n",
        "we can do the same with the Llma index 2 also."
      ],
      "metadata": {
        "id": "vqfN93p4RCGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How to use openai via langchain\n",
        "2. Prompt Templating\n",
        "3. Chains\n",
        "4. Agents - Serp API - Google Search API\n",
        "5. Memory\n",
        "6. Document Loader\n",
        "7. HugggingFace\n",
        "\n",
        "\n",
        "Limitations of OpenAI API:\n",
        "1. It is not free\n",
        "2. It is having limited knowledge.\n",
        "\n",
        "Features of Langchain:\n",
        "1. By using Langchain we can access different LLM's models by using different API.\n",
        "2. you can access private data scources.\n",
        "3. you can access third party API.\n",
        "\n",
        "Langchain:\n",
        "1. Chains\n",
        "2. Agents\n",
        "3. HuggingFace and OpenAI\n",
        "4. Document Loader\n",
        "5. Retain the Memory\n",
        "6. Prompt Templates\n",
        "\n",
        "\n",
        "Documentation: https://python.langchain.com/docs/get_started/introduction\n"
      ],
      "metadata": {
        "id": "SgfEbtA44Cqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Templates:\n",
        "\n",
        "we can construct the prompt based on the input variable, which means the prompt is already predifined but the user has the ability to choose the vairables in the prompts.\n",
        "\n",
        "for example: can you tell me the capital of (India)?. Here, India is the variable."
      ],
      "metadata": {
        "id": "H2RcIGKGGc-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent:\n",
        "\n",
        "We use this agent to call any third party tools.\n",
        "\n",
        "(we can use serp api for extracting information in real time.\n",
        "now by using this serp api we can call google-search-engine.\n",
        "and extract information.)\n",
        "\n",
        "Agent Consists of 2 things:\n",
        "Client: LLM Model.\n",
        "tool: API Services, Wikipedia API, etc.,\n"
      ],
      "metadata": {
        "id": "l6ChWqWuJY0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain:\n",
        "\n",
        "Central to Langchain is a vital component known as LangChain Chains, forming the core connection among one or several Large Language Model's (LLM's). in certain sophisticated applications, it becomes necessary to chain LLMs together, either with each other or with other elements.\n",
        "\n",
        "It consists of 2 components:\n",
        "1. Client,\n",
        "2. Prompt Template\n",
        "\n",
        "If we want to combine multiple chain and set a sequance for that we use simplesequential chain:\n",
        "\n",
        "In this we are using multiple prompts which are interlinked and getting answer.\n",
        "for example: suggest me a name for the startup, which falls under AI.\n",
        "now generate strategies for launching that name startup.\n",
        "\n"
      ],
      "metadata": {
        "id": "J1c7jN96zMvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document Loader:\n",
        "\n",
        "To read any kind of a document.\n"
      ],
      "metadata": {
        "id": "SGUYqhsC9AcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory:\n",
        "\n",
        "The chatbot remembers the conversations we had previosuly.\n",
        "\n",
        "Conversation Buffer Memory:\n",
        "\n",
        "We can attach memory to remember all previous conversation.\n",
        "\n",
        "Conversation Chain:\n",
        "\n",
        "Conversation buffer memory goes growing endlessly.\n",
        "\n",
        "Just remember last 5 Conversation Chain.\n",
        "\n",
        "Just remember last 10-20 Conversation Chain.\n",
        "\n",
        "\n",
        "Converesation Buffer Window Memory:\n",
        "\n",
        "This is the combination of Memory and Converstaion Buffer Memory\n",
        "\n"
      ],
      "metadata": {
        "id": "0gS1FRm3-Hgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**HuggingFace:**\n",
        "\n",
        "Step:1: Install All the required Libraries.\n",
        "\n",
        "Step:2: Import All the Required Libraries.\n",
        "\n",
        "Step:3: Setting the Environment.\n",
        "\n",
        "Here, you can get the token from the Huggingface.\n",
        "\n",
        "Step:4: Models.\n"
      ],
      "metadata": {
        "id": "YC7d4Vn5RIxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for using Huggingface Pipeline: Using Transformer\n",
        "\n",
        "Step:1: Tokenization.\n",
        "\n",
        "Step:2: Model.\n",
        "\n",
        "Step:3: Pipeline.\n",
        "\n",
        "Step:4: HuggingFace Pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "Oezka_a-MnhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Project End to END"
      ],
      "metadata": {
        "id": "9bA5rfhiwC_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative AI Project:\n",
        "\n",
        "MCQ Generator Using OpenAI & LangChain:\n",
        "\n",
        "\n",
        "Step:1: Setup the development Environment.\n",
        "\n",
        "VScode.\n",
        "1. Open Terminal - for creating a folder(mkdir) and the name of the folder - move into the folder(cd and the folder name)- launch VSCode (code .).\n",
        "2. Initialize the Git- this will initialize git in the local folder and so that we can upload the same folder on the github (git init).\n",
        "3. Create a Virtual Environment - (Conda create -p environment name and the python version -y).\n",
        "4. Activate the environment- (Conda activate environment name).\n",
        "5. Ignore any folder or file - (touch .gitignore) - for ignoring the specific folder or files that are not required to upload to thr github.\n",
        "6. Requirement file - (requirement.txt) - openai, langchain, streamlit, python-dotenv, pyPDF2.\n",
        "7. Installing Local Package in the virtual environment- (setup.py).\n",
        "8. Source Code folder -(src folder - (__init__.py file), (mcq folder - (__init__.py))).\n",
        "9. Experiment folder - Jupiter Notebook - (mcq.ipnb file).\n",
        "10. code for (setup.py file)- look for the code below.\n",
        "11. for installing: packages - (pip install package name), all the requirements - (pip install -r requirement.txt), install in the local environment - (python setup.py).\n",
        "12. Findout what are all the packages installed in the local environment - (pip list).\n",
        "13. Importing packages in the ipynb file - os, json, pandas, traceback. look for the code.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Step:2: Few Experiments in Jupiter Notebook.\n",
        "\n",
        "Step:3: Modular Coding.\n",
        "\n",
        "Step:4: Web API Steamlit.\n",
        "\n",
        "Step:5: Deployment.\n"
      ],
      "metadata": {
        "id": "NlS1yHpXa4hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup.py file"
      ],
      "metadata": {
        "id": "3AUpiKy0wJt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from setuptools import setup, find_packages,setup\n",
        "\n",
        "setup(\n",
        "    name='mcq generator',\n",
        "    version='0.0.1',\n",
        "    author = 'Sesha',\n",
        "    author_email = 'seshasaikrishna.redbull@gmail.com',\n",
        "    install_requires=['openai','langchain','streamlit','python-dotenv','pyPDF2'],\n",
        "    packages=find_packages()\n",
        ")"
      ],
      "metadata": {
        "id": "V-QcjlKHwAuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mcq.ipnb file"
      ],
      "metadata": {
        "id": "OGFbik5x0Ry6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import traceback\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv() #load environment variables from .env file"
      ],
      "metadata": {
        "id": "4Tmmz3p10XPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "A9r4TeKs0dwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(openai_api_key = key, model_name = \"gpt-3.5-turbo\",temperature=0.5)"
      ],
      "metadata": {
        "id": "DvLDSWjK0oUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#for loading all the variables from the .env file"
      ],
      "metadata": {
        "id": "oB8gwO1C061z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project: Jupiter Notebook code\n"
      ],
      "metadata": {
        "id": "wtAIkx1QKKTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain .llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import pyPDF2"
      ],
      "metadata": {
        "id": "-KxB2X8yKMyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESPONSE_JSON =\n",
        "{\n",
        "    \"1\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "    },\n",
        "     \"2\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "},\n",
        "     \"3\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "},\n",
        "}"
      ],
      "metadata": {
        "id": "Y2ZIG92vRbWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATE=\"\"\"\n",
        "Text:{text}\n",
        "you are an expert MCQ maker. Given the above text, it is your job to \\\n",
        "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
        "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
        "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
        "Ensure to make {number} MCQs\n",
        "### RESPONSE_JSON\n",
        "{response_json}\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "PR6oJSNYQCyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz_generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"number\",\"subject\",\"tone\",\"response_json\"],\n",
        "    template=TEMPLATE\n",
        ")"
      ],
      "metadata": {
        "id": "Jxk_3QPdKoDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a LLM Chain\n",
        "quiz_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=quiz_generation_prompt,\n",
        "    output_key=\"quiz\",\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "3ozvvoRlSX0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPLATE2 = \"\"\"\n",
        "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
        "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity\n",
        "if the quiz is not at per with the cognitive and anlytical abilities of the students,\\\n",
        "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the students ability.\n",
        "Quiz_MCQs:\n",
        "{quiz}\n",
        "\n",
        "Check from an expert English Writer of the above quiz:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xlDOpuqUSmiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz_evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"subject\",\"quiz\"],\n",
        "    template=TEMPLATE\n",
        ")"
      ],
      "metadata": {
        "id": "tfY6cZ39TxEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=quiz_evaluation_prompt,\n",
        "    output_key=\"review\",\n",
        "    verbose=True)"
      ],
      "metadata": {
        "id": "3snCwkW-UEBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_evaluation_chain = SequentialChain(\n",
        "    chains=[quiz_chain, review_chain],\n",
        "    input_variables=[\"text\", \"number\",\"subject\",\"tone\",\"response_json\"],\n",
        "    output_variables=[\"quiz\",\"review\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "ewdERiGKUXV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"file_path\""
      ],
      "metadata": {
        "id": "emciBeJUVqFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for loading the data, from which the quizes should be generated. create a file name data.txt\n",
        "with open(file_path, \"r\") as f:\n",
        "    TEXT = file.read()"
      ],
      "metadata": {
        "id": "WCmgVa0fUzQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Serialize the Python dictionary into a JSON-formatted string\n",
        "json.dump(RESPONSE_JSON)"
      ],
      "metadata": {
        "id": "QjArlmj7Vtn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUMBER = 3\n",
        "SUBJECT = \"Machine Learning\"\n",
        "TONE = \"simple\""
      ],
      "metadata": {
        "id": "lTQunPcgXVf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#http://python.lanchain.com/docs/modules/model_io//llms/token_usage_tracking\n",
        "\n",
        "#How to setup Token Uage Tracking in LangChain\n",
        "with get_openai_callback() as cb:\n",
        "  response = generate_evaluate_chain(\n",
        "      {\n",
        "          \"text\": TEXT,\n",
        "          \"number\": NUMBER,\n",
        "          \"subject\": SUBJECT,\n",
        "          \"tone\": TONE,\n",
        "          \"response_json\": json.dumps(RESPONSE_JSON),\n",
        "      }\n",
        "  )"
      ],
      "metadata": {
        "id": "kA1TPPXNWGSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Tokens: {cb.total_tokens}\")\n",
        "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "print(f\"Total Cost: {cb.total_cost}\")"
      ],
      "metadata": {
        "id": "w4RRZHeCXroG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "WkCnuAAhYCfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz = response.get(\"quiz\")"
      ],
      "metadata": {
        "id": "sjMQHiT-YJGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz = json.load(quiz)"
      ],
      "metadata": {
        "id": "O78uNdw_YO9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz_tacle_data =[]\n",
        "for key, value in quiz.items():\n",
        "    mcq = value[\"mcq\"]\n",
        "    options = \" | \".join(\n",
        "        [\n",
        "            f\"{option}: {option_value}\"\n",
        "            for option, option_value in value [\"options\"].items()\n",
        "        ]\n",
        "    )\n",
        "    correct = value[\"correct\"]\n",
        "    quiz_table_data.append({\"MCQ\":mcq, \"Choice\": options, \"Correct\": correct})"
      ],
      "metadata": {
        "id": "ZZC4ZtUnYmNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz = pd.DataFrame(quiz_table_data)"
      ],
      "metadata": {
        "id": "iSDVD84jZl4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz.to_csv(\"Machinelearning.csv\",index=False)"
      ],
      "metadata": {
        "id": "0EBuQ7uiZt9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular Codeing"
      ],
      "metadata": {
        "id": "prL3vnHRFIbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps for Modular codeing:\n",
        "1. Creating a logger file - Inside the mcqgenerator folder - (logger.py) - It is file which will help us to save the logs.\n",
        "2. Creating a Utils file - Inside the mcqgenerator folder - (utils.py) - It is a utility file or a helper file.\n",
        "3. Actual Code File - Inside the ,cqgenerator folder - (MCQGenerator.py).\n",
        "4. Response.json file in the root directory.\n",
        "5. StreamLitAPP.py file in the root directory.\n"
      ],
      "metadata": {
        "id": "91_cOSeCFuNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inside the Logger File"
      ],
      "metadata": {
        "id": "9ycTxSQUMMW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "LOG_FILE = f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\"\n",
        "logs_path = os.path.join(os.getcwd(), \"logs\")\n",
        "os.makedirs(logs_path, exist_ok=True)\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(logs_path, LOG_FILE)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "        filename = LOG_FILEPATH,\n",
        "        formate=\"[%(asctime)]s %(name)s - %(levelname)s - %(message)s\")"
      ],
      "metadata": {
        "id": "vfgzBAJzFLyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inside the MCQGenerator File"
      ],
      "metadata": {
        "id": "FjSDXECdayhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import traceback\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from src.mcqgenerator.utils import read_file,get_table_data\n",
        "from src.mcqgenerator.logger import logging\n",
        "\n",
        "#LangChain\n",
        "from lanchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SequentialChain\n",
        "from langchains import LLMChain\n",
        "\n",
        "#Load environment variable from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "key=os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key = key, model_name = \"gpt-3.5-turbo\",temperature=0.3)\n",
        "\n",
        "template=\"\"\"\n",
        "Text:{text}\n",
        "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
        "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
        "Make sure the questions are not reapeated and check all the questions to be confirming the text as well.\n",
        "Make sure to format your response like RESPONSE_JSON below and use it as a guide. \\\n",
        "Ensure to make {number} MCQs\n",
        "\n",
        "###RESPONSE_JSON\n",
        "{response_json}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "quiz_generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "quiz_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\",verbose=True)\n",
        "\n",
        "\n",
        "template2=\"\"\"\n",
        "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students. \\\n",
        "YOu need to evaluate the complxity of the question and give a complete analysis of the quiz. Only use at max 50 words for the complexity analysis\n",
        "if the quiz is not at par with the cognitive and analytical abilities of the students,\\\n",
        "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the students abiltiies\n",
        "Quiz_MCQs:\n",
        "{quiz}\n",
        "\n",
        "Check from an expert English Writer of the above quiz:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "quiz_evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"subject\",\"quiz\"],\n",
        "    template=template2,\n",
        ")\n",
        "\n",
        "review_chain = LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"review\",verbose=True)\n",
        "\n",
        "#This is an Overall Chian where we run the two chains in sequance\n",
        "generate_evaluate_chain = SequentialChain(\n",
        "    chains=[quiz_chain, review_chain],\n",
        "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
        "    output_variables=[\"quiz\", \"review\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd4bai3Ha3O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inside utils.py file"
      ],
      "metadata": {
        "id": "gg8-7W92hdL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import json\n",
        "import traceback\n",
        "\n",
        "def read_file(file):\n",
        "  if file.name.endswith((\".pdf\")):\n",
        "    try:\n",
        "      pdf_reader= PyPDF2.PdfReader(file)\n",
        "      text=\"\"\n",
        "      for page in pdf_reader.pages:\n",
        "        text+=page.extract_text()\n",
        "      return text\n",
        "\n",
        "    except Exception as e:\n",
        "      logging.error(f\"Error reading file: {e}\")\n",
        "\n",
        "  elif file.name.endswith(\".txt\"):\n",
        "    return file.read().decode(\"utf-8\")\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"File format not supported\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_table_data(quiz_str):\n",
        "  try:\n",
        "    # convert the quiz from a str to dict\n",
        "    quiz_dict = json.loads(quiz_str)\n",
        "    quiz_table_data=[]\n",
        "\n",
        "\n",
        "    # iterate over the quiz dictionary and extract the required information\n",
        "    for key,value in quiz_dict.items():\n",
        "      mcq=value[\"mcq\"]\n",
        "      option =\" || \".join(\n",
        "          [f\"{option} - > {option_value}\" for option,option_value in value[\"options\"].items()]\n",
        "      )\n",
        "      correct= value[\"correct\"]\n",
        "      quiz_table_data.append({\n",
        "          \"MCQ\":mcq,\n",
        "          \"Choices\":option,\n",
        "          \"Correct\":correct\n",
        "      })\n",
        "\n",
        "    return quiz_table_data\n",
        "\n",
        "  except Exception as e:\n",
        "    traaceback.print_exception(type(e), e, e.__traceback__)\n",
        "    return False"
      ],
      "metadata": {
        "id": "0_wu4ygJbfwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inside Response.json"
      ],
      "metadata": {
        "id": "u-z8B8YBkh9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    \"1\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "    },\n",
        "     \"2\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "},\n",
        "     \"3\": {\n",
        "        \"mcq\": \"Multiple choice question\",\n",
        "        \"options\" : {\n",
        "            \"a\": \"option a\",\n",
        "            \"b\": \"option b\",\n",
        "            \"c\": \"option c\",\n",
        "            \"d\": \"option d\",\n",
        "        },\n",
        "        \"correct\": \"correct answer\",\n",
        "},\n",
        "}"
      ],
      "metadata": {
        "id": "Lul_-xJaknd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inside StreamlitAPP.py file"
      ],
      "metadata": {
        "id": "gaGB5j4sk9Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import traceback\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from src.mcqgenerator.utils read_file,get_table_data\n",
        "import streamlit as st\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from src.mcq.generator.MCQGenerator import generate_evaluate_chain\n",
        "from src.mcq.generator.logger import logging\n",
        "\n",
        "#loading json file\n",
        "with open('Response.json file path','r') as file:\n",
        "  RESPONSE_JSON = json.load(file)\n",
        "\n",
        "creating a title for the app\n",
        "st.title(\"MCQ Creator Application with LangChain\")\n",
        "\n",
        "#Create a form using st.form\n",
        "with st.form(\"user_inputs\"):\n",
        "  #File Upload\n",
        "  uploaded_file=st.file_uploader(\"Upload a PDF or txt file\")\n",
        "\n",
        "  #Input Fields\n",
        "  mcq_count=number_input(\"No. of MCQs\",min_value=3, max_value=50)\n",
        "\n",
        "  #Subject\n",
        "  subject=st.text_input(\"Insert Subject\", max_chars=20)\n",
        "\n",
        "  #Quiz Tone\n",
        "  tone=st.text_input(\"Complexity Level of Questions\",max_chars=20, placeholder=\"Simple\")\n",
        "\n",
        "  #Add Button\n",
        "  button=st.form_submit_button(\"Generate MCQ\")\n",
        "\n",
        "  #Check if the button is clicked and all fields have input\n",
        "\n",
        "  if button and uploaded_file is not None and mcq_count and subject and tone:\n",
        "    with st.spinner(\"loading...\"):\n",
        "      try:\n",
        "        text=read_file(uploaded_file)\n",
        "        #Count tokens and the cost of API call\n",
        "        with get_openai_callback() as cb:\n",
        "          response=generate_evaluate_chain(\n",
        "              {\n",
        "                  \"text\":text,\n",
        "                  \"mcq_count\":mcq_count,\n",
        "                  \"subject\":subject,\n",
        "                  \"tone\":tone\n",
        "\n",
        "                  \"response_json\": json.dumps(RESPONSE_JSON)\n",
        "              }\n",
        "          )\n",
        "\n",
        "        #st.write(response)\n",
        "\n",
        "      except Exception as e:\n",
        "        traaceback.print_exception(type(e), e, e.__traceback__)\n",
        "        st.error(\"Error\")\n",
        "\n",
        "      else:\n",
        "        print(f\"Total Tokens:{cb.total_tokens}\")\n",
        "        print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
        "        print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
        "        print(f\"Total Cost:{cb.total_cost}\")\n",
        "        if isinstance(response, dict):\n",
        "          #Extract the quiz data from the response\n",
        "          quiz=response.get(\"quiz\",None)\n",
        "          if quiz is not None:\n",
        "            table_data=get_table_data(quiz)\n",
        "            if table_data is not None:\n",
        "              df=pd.DataFrame(tanle_data)\n",
        "              df.index=df.index+1\n",
        "              st.table(df)\n",
        "              #Display the review in atext box as well\n",
        "              st.text_area(label = \"Review\", value= response[\"review\"])\n",
        "            else:\n",
        "              st.error(\"Error in the table data\")\n",
        "\n",
        "          else:\n",
        "            st.write(resonse)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWISQOzSlBQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWS: Deployment"
      ],
      "metadata": {
        "id": "DcX--w-N9v8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. AWS - EC2 Ubuntu\n",
        "2. Github"
      ],
      "metadata": {
        "id": "ce-O2EfB91Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating the ubunto machine on AWS EC2 instance:\n",
        "1. login to the aws website.\n",
        "2. search about the ec2 instance.\n",
        "3. you need to config the UBUNTU Machine.\n",
        "4. Launch the instance.\n",
        "5. sudo apt update.\n",
        "6. sudo apt-get update.\n",
        "7. sudo apt upgrade -y.\n",
        "8. sudo apt install git curl unzip tar make sudo vim wget -y.\n",
        "9. git clone and the link of the repository.\n",
        "10. cd and the application name.\n",
        "11. touch .env.\n",
        "12. vi .env. _ opens vi editor.\n",
        "13.  Press insert on the keyboard.\n",
        "14. paste the environment variables.\n",
        "15. press the esc and :wq. for exiting the vi editor.\n",
        "16. sudo apt install python3-pip.\n",
        "17. pip3 install -r requirements.txt.\n",
        "18. python3 -m streamlit run StreamlitAPP.py."
      ],
      "metadata": {
        "id": "PeQD8QvN-X-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Database"
      ],
      "metadata": {
        "id": "r8uP7vPaMyAt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector are nothing but set of numbers and store it in the vector database.\n",
        "1. What is Vector Database?\n",
        "2. Why we need Vector DB?\n",
        "3. How Vector DB work?\n",
        "4. Use cases of Vector DB.\n",
        "5. Some widely used Vector DB.\n",
        "6. Practical demo. using Python & Langchain."
      ],
      "metadata": {
        "id": "N_tfjq9JM1am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Vector database is a database used for storing high-dimensional vectors such as word embeddings and Image embeddings.\n",
        "\n",
        "Encoding: Text to Numeric.\n",
        "1. Without Deep Learning: (Sparse Matric, No Context)\n",
        "\n",
        "Document Matrix or Bag of words, TF-IDF, N-Gram, One Hot Encodding, Interger Encoding,etc.,\n",
        "2. With Deep Learning: Embedding(Dense Vector, Context Full)\n",
        "\n",
        "Word to Vector, Fast Text, ELMO, BERT, Glove,etc.,\n",
        "\n",
        "In Embedding we will create 2 things:\n",
        "Vacob and Features.\n",
        "and the combination of this both will give a weight vector, between 0-1.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmoOMeZznKf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why we need Vector DB?\n",
        "\n",
        "Over 80 - 85% data out there is unstructured data.\n",
        "\n",
        "We cannot store them easily into a Relational/Traditional Database.\n",
        "\n",
        "Use Cases of Vector DB:\n",
        "\n",
        "1. Long-Term memory for LLMs.\n",
        "2. Semantic Search: Search based on the menaing of context.\n",
        "3. Similarity Search: Text, Images, Videos, Audios.\n",
        "4. Recommendation engines as well.\n",
        "\n",
        "Some Widely used Vector DB:\n",
        "1. Chroma.\n",
        "2. Weaviate.\n",
        "3. Pinecone.\n",
        "4. Redit.\n",
        "5. FASISS."
      ],
      "metadata": {
        "id": "ohbH9KblwezK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone DB"
      ],
      "metadata": {
        "id": "htjaR4IZzqfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Answering from the PDF's"
      ],
      "metadata": {
        "id": "K7ubm3fvzusm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lanchain\n",
        "!pip install pinecone-client\n",
        "!pip install PyPDF2\n",
        "!pip install openai\n",
        "!ipip install tiktoken\n"
      ],
      "metadata": {
        "id": "fjBds3OWzzbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from lanchain.llms import OpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ],
      "metadata": {
        "id": "AKcphGkx1GMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs\n",
        "#for creating the folder inside the working directory and save the pdf or text data in the directory"
      ],
      "metadata": {
        "id": "TPpwq_5Q2ZjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")"
      ],
      "metadata": {
        "id": "kr4gLlM42cUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "DLdyAHNH3fXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
      ],
      "metadata": {
        "id": "rp8XoY_s3m5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "Tv_jTUfb4AH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "3Pfnd3fX4Q9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "7Ww6Rz0K5Rna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.embed_query(\"what is the meaning of life\")"
      ],
      "metadata": {
        "id": "w30UAMmK5VSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "PINECONE_API_ENV = os.getenv(\"PINECONE_API_ENV\")"
      ],
      "metadata": {
        "id": "SdxbtQh65xA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "omport pinecone"
      ],
      "metadata": {
        "id": "bBFDIp3U6USd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)"
      ],
      "metadata": {
        "id": "KyVmUKzP6WoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"text\" #put in the name of your pinecone index here"
      ],
      "metadata": {
        "id": "MO__lER46qZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)"
      ],
      "metadata": {
        "id": "TzF-9FDM64oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Embeddings for each of the text chunks\n",
        "docsearch = Pinecone.from_text([t.page_content for t in text_chunks], embedding, index_name=index_name)"
      ],
      "metadata": {
        "id": "-2rcIv9o8CoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi, how are you?\" #finding the similarity search of this statement with the data in the database"
      ],
      "metadata": {
        "id": "DVxttJTg8SBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "zyOKK4qz80EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()"
      ],
      "metadata": {
        "id": "y7OSaT3L9CzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "5QBEV9Ev9MbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(query)"
      ],
      "metadata": {
        "id": "omujVgWK9Y87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt:\")\n",
        "  if user_input == 'exit' :\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input:\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ],
      "metadata": {
        "id": "AMNRmT9X9xs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chroma DB"
      ],
      "metadata": {
        "id": "Ui2cuyrl41Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install chromadb openai langchain tiktoken"
      ],
      "metadata": {
        "id": "RFg6KG4v45MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show chromadb"
      ],
      "metadata": {
        "id": "UYdNm0UTB7fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q #and the link of the data you want to use"
      ],
      "metadata": {
        "id": "49fpQ5J3CXQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q #(the name of the zip file) -d (and the name of the unzip file)\n",
        "#to unzip the datafile"
      ],
      "metadata": {
        "id": "7kCcf0NSC2tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "KXjtzYz9DUeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "TYa2S2fkDwA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader(\"Path to the data\", glob = \"./*.txt\", loader_cls=TextLoader)"
      ],
      "metadata": {
        "id": "9K_m2E2bEhJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = loader.load()"
      ],
      "metadata": {
        "id": "gjdvG_AzFePm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "NXxINFBxF9Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[1].page_content"
      ],
      "metadata": {
        "id": "otPeFtSrR3FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating DB\n",
        "from langchain import embeddings\n",
        "#for creating a folder to store the embeddings\n",
        "persist_directory = \"db\"\n",
        "\n",
        "embeddings = embeddings.OpenAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "FOOjPEgiR8kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None"
      ],
      "metadata": {
        "id": "fJAqNmKdSrY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can load the persisted database from disk, and use it as a normal one\n",
        "vectordb = Chrome(persist_directory=persist_directory,embedding_function=embeddings)"
      ],
      "metadata": {
        "id": "IWriHi30Tq-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a retriever\n",
        "retriever = vectordb.as_retriever() #search_kwargs={\"k\":2} for limited output"
      ],
      "metadata": {
        "id": "6m9n1JtBT6QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is the meaning of life?\")"
      ],
      "metadata": {
        "id": "4bLbTfgaUiSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a chain\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "metadata": {
        "id": "20v41HQpUln8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()"
      ],
      "metadata": {
        "id": "3X49eABEWo_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True)"
      ],
      "metadata": {
        "id": "F21u_FIMWrHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "  print(llm_response[\"resuult\"])\n",
        "  print('\\n\\nSources:')\n",
        "  for source in llm_response[\"source_documents\"]:\n",
        "    print(source.metadata[\"source\"])"
      ],
      "metadata": {
        "id": "3bG2dfTzXEf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#full example\n",
        "query = \"How much money did Microsoft raise?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "d1Il5q-pXoJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 2"
      ],
      "metadata": {
        "id": "8J5d1Dq0EQmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github for llms: https://github.com/eugeneyan/open-llms\n",
        "\n",
        "1. Introduction to Llama 2.\n",
        "2. How to run Llama 2.\n",
        "3. How to use Llama 2 with Langchain.\n",
        "4. How to build Generative AI projects using Llama 2.\n",
        "\n",
        "Quantization Methods: GGML, etc.,"
      ],
      "metadata": {
        "id": "pz81TSolE7Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS= \"DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy ==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.4"
      ],
      "metadata": {
        "id": "MU9oAezFE444"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13b-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_0.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "2WEv-HDGPSKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "rZOcMryOP7Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "KriyL8PVQDpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path= model_path,\n",
        "    n_threads = 2, #CPU cores\n",
        "    n_batch=512 #Should be between 1 and n_ctx, consider the amount of VRAM in your GPU\n",
        "    n_gpu_layers=32 #Change this value based on your model and your GPU VRAM pool.\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "p4f25tANQPzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a linear regression code\"\n",
        "prompt_template = f\"\"\" SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0g-Me6w6RxbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(prompt = prompt_templete, max_tokens=256, temperature = 0.5, top_p =0.95, repeat_penalty = 1.2, top_k = 150, echo = True )"
      ],
      "metadata": {
        "id": "7NgGoMZRSgmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "0WpTriBmS1YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "yqwa-0xZUMZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 2 with Langchain"
      ],
      "metadata": {
        "id": "Br5kMkVzVQTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "id": "Xy5aMBAbVPYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logging into Hugging Face account\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "DYAqwvoEPTn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import all the required libraries\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import tourch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "fqLZGmYOPlop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Llama 2 Model:\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "4U0ynTUuQGAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype = torch.bfloat16,\n",
        "    trust_remote_code = True,\n",
        "    device_length= \"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k =10,\n",
        "    num_return_sequences =1,\n",
        "    eos_token_id=toeknizer.eos_token_id\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Ba4xykCKQ4zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipeline,model_kwargs={'temperature':0})\n",
        "prompt = \"what would be a good name for a company that makes coloful socks?\"\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "cZTUVURIRq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Templates.\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "a4CeCUVSTlqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplates(input_variables = [\"cuisine\",\n",
        "                                                     template = \"I want to open a restaurant for {cuisine } food. Suggest a fency name for this\"])\n",
        "input_prompt = prompt_template.format(cuisine = \"Indian\")\n",
        "print(input_prompt)"
      ],
      "metadata": {
        "id": "U1Cly-ByTs39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}